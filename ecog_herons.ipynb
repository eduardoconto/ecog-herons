{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"ecog_herons.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","source":["# Data description"],"metadata":{"id":"qm2u0iwTLifO"}},{"cell_type":"markdown","source":["This is one of multiple ECoG datasets from Miller 2019, recorded in a clinical settings with a variety of tasks. Raw data and dataset paper are here:\n","\n","https://exhibits.stanford.edu/data/catalog/zk881ps0522\n","https://www.nature.com/articles/s41562-019-0678-3\n","\n","This particular dataset was originally described in this paper: \n","\n","*Miller, Kai J., Gerwin Schalk, Eberhard E. Fetz, Marcel Den Nijs, Jeffrey G. Ojemann, and Rajesh PN Rao. \"Cortical activity during motor execution, motor imagery, and imagery-based online feedback.\" Proceedings of the National Academy of Sciences (2010): 200913697.*\n","\n","`dat1` and `dat2` are data from the two blocks performed in each subject. The first one was the actual movements, the second one was motor imagery. For the movement task, from the original dataset instructions:\n","\n","*Patients performed simple, repetitive, motor tasks of hand (synchronous flexion and extension of all fingers, i.e., clenching and releasing a fist at a self-paced rate of ~1-2 Hz) or tongue (opening of mouth with protrusion and retraction of the tongue, i.e., sticking the tongue in and out, also at ~1-2 Hz). These movements were performed in an interval-based manner, alternating between movement and rest, and the side of move- ment was always contralateral to the side of cortical grid placement.*\n","\n","For the imagery task, from the original dataset instructions:\n","\n","*Following the overt movement experiment, each subject performed an imagery task, imagining making identical movement rather than executing the movement. The imagery was kinesthetic rather than visual (“imagine yourself performing the actions like you just did”; i.e., “don’t imagine what it looked like, but imagine making the motions”).*\n","\n","Sample rate is always 1000Hz, and the ECoG data has been notch-filtered at 60, 120, 180, 240 and 250Hz, followed by z-scoring across time and conversion to float16 to minimize size. Please convert back to float32 after loading the data in the notebook, to avoid unexpected behavior. \n","\n","Both experiments: \n","* `dat['V']`: continuous voltage data (time by channels)\n","* `dat['srate']`: acquisition rate (1000 Hz). All stimulus times are in units of this.  \n","* `dat['t_on']`: time of stimulus onset in data samples\n","* `dat['t_off']`: time of stimulus offset, always 400 samples after `t_on`\n","* `dat['stim_id`]: identity of stimulus (11 = tongue, 12 = hand), real or imaginary stimulus\n","* `dat['scale_uv']`: scale factor to multiply the data values to get to microvolts (uV). \n","* `dat['locs`]`: 3D electrode positions on the brain surface"],"metadata":{"id":"a-AjegnLLifP"}},{"cell_type":"markdown","source":["# Project Proposal"],"metadata":{"id":"8PaBv268LifR"}},{"cell_type":"markdown","source":["## Distinguishing motor execution from motor imagery\n","\n","Keywords: motor execution, motor imagery, ECoG, ECoG decoding, event related potential, power spectrum density, hand control, tongue control, motor cortex, Miller 2019, efficient real-time decoding. \n","\n","We propose to understand the neurological differences between actual movement and motor imagery. Furthermore, we would like to decode signals from the motor cortex to predict movement onset. To achieve this, we will leverage the ECoG dataset from Miller 2019 on motor imagery. \n","\n","Our research will focus in two questions: \n","1. How is motor execution different from motor imagery? \n","2. Can we decode hand movement and tongue movement? \n","\n","More precisely, for (1), we ask whether there exists a model that can differentiate between motor execution and motor imagery? Does it generalize across the two studied movement tasks (tongue and hand)? Our initial hypothesis is that motor execution should active a larger number of areas in the brain, since it has to (a) decide on precise movement to execute and (b) execute it by recruiting the spinal cord. Is this hypothesis backed by the data? In general, what are the brain mechanisms in play here?  \n","\n","For (2), our goal is to decode the hand movement and tongue movement with an efficient model. Can we achieve a high decoding accuracy (above 90%)? What is the minimum number of parameters required for such decoding (best trade-off number of parameters vs efficiency)? Can we build an efficient decoding algorithm suitable for a real-time usage? Can we predict the motor task prior to its onset and, if so, by how much time?  \n","\n","We will attempt to tackle these questions by analysis the dataset: preprocessing, extracting features (Power Spectrum Density, ERP, electrode position) and building suitable models."],"metadata":{"id":"k1r0RV_ILifS"}},{"cell_type":"markdown","source":["# Data analysis"],"metadata":{"id":"6rZk_d2GLifT"}},{"cell_type":"markdown","source":["## Utils "],"metadata":{"id":"_ebk4Uc3LifU"}},{"cell_type":"code","execution_count":null,"source":["#@title Utils\n","\n","import os, requests\n","import numpy as np\n","from matplotlib import rcParams\n","from matplotlib import pyplot as plt\n","from scipy import signal\n","!pip install mne\n","import mne\n","import numpy as np\n","import scipy\n","\n","rcParams['figure.figsize'] = [20, 4]\n","rcParams['font.size'] =15\n","rcParams['axes.spines.top'] = False\n","rcParams['axes.spines.right'] = False\n","rcParams['figure.autolayout'] = True\n","\n","alldat = ''\n","dataLoaded = False\n","fname = 'motor_imagery.npz'\n","\n","def _fetch_data():\n","    ''' Download data '''\n","\n","    url = \"https://osf.io/ksqv8/download\"\n","    if not os.path.isfile(fname):\n","        try:\n","            r = requests.get(url)\n","        except requests.ConnectionError:\n","            print(\"!!! Failed to download data !!!\")\n","        else:\n","            if r.status_code != requests.codes.ok:\n","                print(\"!!! Failed to download data !!!\")\n","            else:\n","                with open(fname, \"wb\") as fid:\n","                    fid.write(r.content)\n","\n","def get_data(subject, experiment):\n","    ''' Return the data\n","\n","        Input:\n","          subject: subject id\n","          expeirment: experiment number (0 = real; 1 = imaginary)\n","    '''\n","    global dataLoaded, alldat\n","    _fetch_data()\n","\n","    if dataLoaded == False:\n","      alldat = np.load(fname, allow_pickle=True)['dat']\n","      dataLoaded = True\n","\n","    dat = alldat[subject][experiment]\n","    # convert it to float32 for numerical ops\n","    V = dat[\"V\"].astype(\"float32\")\n","    dat[\"V\"] = V\n","\n","    return dat\n","\n","def preprocess_voltage(V, filter_type):\n","    ''' Filter voltage\n","\n","        Args:\n","            V: voltage\n","            filter_type: 'high' | 'low'\n","\n","        Returns: filtered voltage\n","    '''\n","    # high-pass filter above 50 Hz\n","    # remove the noise\n","    b, a = signal.butter(3, [50], btype=filter_type, fs=1000)\n","    V = signal.filtfilt(b, a, V, 0)\n","\n","    # compute smooth envelope of this signal = approx power\n","    V = np.abs(V) ** 2\n","    b, a = signal.butter(3, [10], btype=\"low\", fs=1000)\n","    V = signal.filtfilt(b, a, V, 0)\n","\n","    # normalize each channel so its mean power is 1\n","    V = V / V.mean(0)\n","\n","    return V\n","\n","def plot_electrodes(dat, filter_type, electrodes):\n","  ''' Plot given electrodes\n","\n","      Args:\n","          dat: data\n","          filter_type: pre-processing filter (high|low)\n","          electrodes: set of electrodes to plot\n","  '''\n","\n","  V_high = preprocess_voltage(dat['V'], filter_type)\n","\n","  # average the broadband power across all tongue and hand trial\n","  nt, nchan = V_high.shape\n","  nstim = len(dat['t_on'])\n","\n","  # TODO why 2000 here is enough for all trials\n","  #trange = np.arange(0, 2000)\n","  BEGIN = 0 # 2000 #0\n","  END = 2000 # 4000\n","  trange = np.arange(BEGIN, END)\n","  ts = dat['t_on'][BEGIN:,np.newaxis] + trange\n","  V_epochs = np.reshape(V_high[ts, :], (nstim, END, nchan))\n","\n","  V_tongue = (V_epochs[dat['stim_id']==11]).mean(0)\n","  V_hand   = (V_epochs[dat['stim_id']==12]).mean(0)\n","\n","  # let's find the electrodes that distinguish tongue from hand movements\n","  # note the behaviors happen some time after the visual cue\n","  from matplotlib import pyplot as plt\n","\n","  plt.figure(figsize=(20,10))\n","  #for j in range(46):\n","  for j in electrodes:\n","    ax = plt.subplot(5,10,j+1)\n","    plt.plot(trange, V_tongue[:,j])\n","    plt.plot(trange, V_hand[:,j])\n","    plt.title('ch%d'%j)\n","    plt.xticks([0, 1000, 2000])\n","    plt.ylim([0, 4])\n","\n","\n","def get_mne_structs(dat, epoch, filter = True):\n","  '''\n","    Return mne structs (raw data and epoched data)\n","    Args:\n","      dat: original numpy data\n","      epoch: (epoch_begin, epoch_end) [time in seconds]\n","  '''\n","\n","  V = dat['V']\n","  locs = dat['locs'] / 1000 # 1000: mm -> m\n","  n_channels = dat['V'].shape[1]\n","  srate = dat['srate']\n","  stim_id = dat['stim_id']\n","  t_on = dat['t_on']\n","\n","  elec_indexes_str = [str(ch) for ch in range(n_channels)]\n","  locs_dict = dict(zip(elec_indexes_str, locs))\n","  montage = mne.channels.make_dig_montage(locs_dict, coord_frame='mni_tal')\n","\n","  info = mne.create_info(n_channels, sfreq=srate, ch_types='ecog')\n","  raw = mne.io.RawArray(V.T, info)\n","  # filtering the raw data\n","  if (filter): \n","    _ = raw.filter(l_freq=76, h_freq=110)\n","  raw.set_montage(montage)\n","\n","  events = np.vstack((t_on.astype(int), np.zeros(t_on.size).astype(int)))\n","  events = np.vstack((events, stim_id.astype(int))).T\n","  event_dict = {'tongue': 11, 'hand': 12}\n","  epochs = mne.Epochs(raw, events, event_id = event_dict,\n","                      tmin = epoch[0], tmax = epoch[1], baseline=None)\n","  return raw, epochs\n","\n","def plot_fft(dat, channel, num_samples):\n","    '''\n","        Plot time vs amplitude & frequency vs power\n","    '''\n","    V = dat['V'][:,channel][0:num_samples]\n","    sampling_rate = dat['srate']\n","    window_length = 51\n","    polyorder = 3\n","\n","    plt.figure(figsize=(12, 12))\n","\n","    plot_rows = 2; plot_cols = 1; plot_idx = 1\n","\n","    plt.subplot(plot_rows, plot_cols, plot_idx); plot_idx = plot_idx + 1\n","    # we see effect of zscoring\n","    plt.plot(signal.savgol_filter(V, window_length, polyorder))\n","\n","    plt.xlabel(\"time\")\n","    plt.ylabel(\"amplitude\")\n","    plt.xlim(0,num_samples)\n","\n","    V_fft = scipy.fft.fft(V)\n","    N = len(V)\n","    n = np.arange(N)  # S (samples)\n","    T = N / sampling_rate  # [S/(S/s)] = s\n","    freq = n / T  # [S/s] = [Hz](V)\n","\n","    logpower = signal.savgol_filter(\n","        np.log(np.abs(V_fft) ** 2), window_length, polyorder)\n","\n","    plt.subplot(plot_rows, plot_cols, plot_idx); plot_idx = plot_idx + 1\n","    plt.plot(freq, logpower)\n","    plt.xlabel('Freq (Hz)')\n","    plt.ylabel('FFT logpower')\n","    plt.xlim(0,150)\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"to2gy6akLifW","executionInfo":{"status":"ok","timestamp":1629136414090,"user_tz":-120,"elapsed":9751,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"29950947-9c2e-4d91-cd65-85057d80cf89"}},{"cell_type":"markdown","source":["## Activation plot in time\n","\n","We'll plot the activation for all electrodes. First we define to functions to preprocess/filter and then a plotting function. \n","\n","* Only patient 0 shows a very distinct activation w/ the provide filter parameters\n","    * as per original notebook, we see activation on electrode 20 for hand and 42 for tongue\n","    * hand = orange\n","    * tongue = blue\n","    * What kind of parameters do work w/ do see some activation for other patients? \n","    * What is different about patient 0?"],"metadata":{"id":"-3YU0DS0LifZ"}},{"cell_type":"code","execution_count":null,"source":["plot_electrodes(get_data(subject=0,experiment=0), 'high', range(46))\n","plot_electrodes(get_data(subject=1,experiment=0), 'high', range(46))"],"outputs":[],"metadata":{"id":"U7o18ZYGLifZ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1629136441316,"user_tz":-120,"elapsed":27236,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"05081471-2a63-4784-e1c7-b7ccb7fb1fff"}},{"cell_type":"code","execution_count":null,"source":["# As low filer, we've more distinction among patients apparently. \n","\n","plot_electrodes(get_data(subject=0,experiment=0), 'low', range(46))\n","plot_electrodes(get_data(subject=1,experiment=0), 'low', range(46))"],"outputs":[],"metadata":{"id":"AMIp8VbiLifa","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1629136459051,"user_tz":-120,"elapsed":17740,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"26fe3fec-6398-452e-9edf-4f578d2b61a7"}},{"cell_type":"markdown","source":["- Comparing the evoked responses across subjects, we see no meaningful pattern emmerging, though we can see some good enough differents for hand and tongue. \n","\n","- Patients 1 and 7 seems to have a particularly noisy data set w/ the analysis and channel combining that was done here (gfp).\n","\n","- gfp. What is this? "],"metadata":{"id":"Sf437tehLifb"}},{"cell_type":"markdown","source":["## Analyzing with MNE"],"metadata":{"id":"F3uZcNlKLifb"}},{"cell_type":"markdown","source":["Beyond gamma band [Miller, 2008]. Desynchronization of the gamma band. \n","Decoding hand vs tongue could be done w/ a very simple number J0\n","\n","Time interval for each recording: \n","* for analysis they take a slot [1, 2.5s], there is some overlap across trials, there is some persistent activity. After you stopped movement, the signal doesn't stop right away \n","* we can some clear distinction in the evoked potential across individuals"],"metadata":{"id":"0IXzt6esLifd"}},{"cell_type":"code","execution_count":null,"source":["for s in range(0,1):\n","  for e in range(2):\n","    dat = get_data(s, e)\n","    \n","    raw, epochs = get_mne_structs(dat, (1,2.5), filter=False)\n","    montage = epochs.get_montage()\n","    #montage.plot()\n","    \n","    evoked_hand = epochs['tongue'].average()\n","    evoked_tongue = epochs['hand'].average()\n","    print(evoked_hand, evoked_tongue)\n","    \n","    mne.viz.plot_compare_evokeds([evoked_hand, evoked_tongue])"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"DoGG0oJdLifd","executionInfo":{"status":"ok","timestamp":1629136459681,"user_tz":-120,"elapsed":644,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"ef3af853-4bd8-4ed3-9756-fbdd9685de30"}},{"cell_type":"code","execution_count":null,"source":["## Other MNE analysis \n","# - fft response \n","# - response in time plot\n","\n","# some patients seem to have a strange montage w/ electrodes messed up\n","\n","for s in range(7):\n","    dat = get_data(s, 0)\n","    \n","    raw, epochs = get_mne_structs(dat, (1,2.5))\n","    montage = epochs.get_montage()\n","    montage.plot()"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"2G1OIj0sLife","executionInfo":{"status":"ok","timestamp":1629136475005,"user_tz":-120,"elapsed":15338,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"1344e534-f871-406e-b8a2-e9f37cac133b"}},{"cell_type":"code","execution_count":null,"source":["# let's do some further pre-processing as saing so far\n","dat = get_data(0, 0)\n","raw, epochs = get_mne_structs(dat, (1,2.5))\n","print(raw)\n","print(epochs)\n","\n","events = epochs.events\n","event_id = epochs.event_id\n","print(events)\n"],"outputs":[],"metadata":{"id":"hZNYzFkyLiff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629136476449,"user_tz":-120,"elapsed":1460,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"dcc479f8-9286-4c90-f696-5c144de9af6c"}},{"cell_type":"code","execution_count":null,"source":["dat = get_data(0, 0)\n","raw, epochs = get_mne_structs(dat, (1,2.5))\n","\n","events = epochs.events\n","event_id = epochs.event_id\n","\n","mne.viz.plot_events(events, sfreq=raw.info['sfreq'], \n","                    first_samp=raw.first_samp, event_id=event_id)\n","\n","epochs['hand'].plot_image()\n","epochs['tongue'].plot_image()\n","evoked_hand.plot(gfp=True)\n","evoked_tongue.plot(gfp=True)\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DDhdphZdLiff","executionInfo":{"status":"ok","timestamp":1629136482781,"user_tz":-120,"elapsed":6349,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"3f521c3b-a81e-4075-d0a9-187c5add6573"}},{"cell_type":"code","execution_count":null,"source":["dat = get_data(0, 0)\n","\n","raw, epochs = get_mne_structs(dat, (1,2.5))\n","\n","print(raw)\n","\n","mne.viz.plot_raw_psd(raw, fmin=76, fmax=100)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"74XKhZwBLifg","executionInfo":{"status":"ok","timestamp":1629136485935,"user_tz":-120,"elapsed":3175,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"418ec1de-e13e-41b8-d827-e71ceebc9b29"}},{"cell_type":"markdown","source":["# Abstract"],"metadata":{"id":"pYmrGaQ_Lifh"}},{"cell_type":"markdown","source":["_**Cross-session decoding accuracy: motor imagery vs. motor execution**_\n","\n","Brain Computer Interfaces (BCI) aim to restore or augment human skills via efficient neuroprosthetic solutions. \n","One common BCI paradigm is motor imagery decoding: a subject is instructed to imagine making a movement without actually executing it.\n","\n","However, few studies so far focused on motor execution. \n","Hence, it is unknown whether decoding algorithms for motor imagery would have a similar accuracy on motor execution. \n","On the other hand, prior research has shown that both motor imagery and execution have similar brain activation patterns. \n","Therefore, we hypothesize that a model trained on motor imagery data would have similar performance on motor execution data. \n","\n","Here we evaluate a traditional BCI pipeline composed of filtering on the high-frequency band (76-100Hz), \n","Principal Component Analysis (PCA) for dimensionality reduction and Support Vector Machine (SVM) for classification. \n","We then evaluate the cross-session accuracy for each subject. \n","We perform such analysis on the data from Miller, 2019, composed of electrocorticography (ECoG) recordings collected from eight patients. \n","\n","We aim to show that, if we train a decoding model on motor imagery data, then we can use it to \n","decode motor execution data and obtain similar accuracy across these two experiments. \n","This would allow us to conclude that, since accuracy is preserved, we can decode ECoG data for both motor execution and imagery tasks using the same model. \n","This would also further validate previous studies showing that the brain activation for motor execution and imagery are similar. \n","\n","Future research should attempt to replicate these findings on other datasets and, in particular, on electroencephalogram (EEG) data. \n","\n","\n"],"metadata":{"id":"7ebQmxabLifh"}},{"cell_type":"markdown","source":["# Data classification\n","\n","Let's just try some naive classification pipeline and see what happens. "],"metadata":{"id":"GPM97W_CLifi"}},{"cell_type":"code","execution_count":null,"source":["# @title Imports \n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.svm import SVC  # \"Support Vector Classifier\"\n","from mne.decoding import CSP"],"outputs":[],"metadata":{"id":"uirINxYrPSEA","executionInfo":{"status":"ok","timestamp":1629136485936,"user_tz":-120,"elapsed":17,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}}}},{"cell_type":"code","execution_count":null,"source":["# Classification pipeline functions\n","\n","def get_classifier_data(epochs):\n","  # Get training data\n","  X = epochs.get_data() # events X channels X value\n","  y = epochs.events[:,-1] # flatten out array\n","  le = LabelEncoder() # label events y between [0,n_class-1]\n","  y = le.fit_transform(y)\n","\n","  return X, y\n","\n","def train_classifier(s, e):\n","  # Get training data\n","  dat = get_data(subject = s, experiment = e)\n","  raw, epochs = get_mne_structs(dat, (0,2)) # filtered data\n","  X, y = get_classifier_data(epochs)\n","\n","  # Build pipeline\n","  csp = CSP(n_components=8, \n","            transform_into='average_power',\n","            reg='ledoit_wolf',\n","            cov_est='epoch',\n","            component_order='mutual_info',\n","            rank='info')\n","  pipeline = make_pipeline(\n","      csp, \n","      SVC(C=1, kernel='linear'))\n","  \n","  # Evaluate pipeline\n","  acc = cross_val_score(pipeline, X, y, cv=5)\n","\n","  # Fit TODO why not fit after cross validatation?\n","  pipeline.fit(X, y)\n","\n","  return pipeline, acc\n","\n","def train_classifier_all_subjects(experiment):\n","  # load training data (T)\n","  pipelines = []\n","  accuracies = []\n","\n","  for s in range(7):\n","    pipeline, acc = train_classifier(s, experiment)\n","\n","    pipelines.append(pipeline)\n","    accuracies.append(acc)\n","\n","  return pipelines, accuracies"],"outputs":[],"metadata":{"id":"_5_lNdxviVVU","executionInfo":{"status":"ok","timestamp":1629136485937,"user_tz":-120,"elapsed":16,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}}}},{"cell_type":"code","execution_count":null,"source":["# Run pipeline for all subjects\n","pipe_motor, acc_motor = train_classifier_all_subjects(0)\n","pipe_img, acc_img = train_classifier_all_subjects(1)"],"outputs":[],"metadata":{"id":"R-6avOh8wkNA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629136795234,"user_tz":-120,"elapsed":309312,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"321f8bcd-56a3-44f5-d35a-d97736cc37c0"}},{"cell_type":"code","execution_count":null,"source":["print(\"Train on motor imagery & validate on motor execution\")\n","print(\"Subject,TestAccuracy\")\n","for s in range(7):\n","  # get validation data\n","  datE = get_data(subject = s, experiment = 0)\n","  rawE, epochsE = get_mne_structs(datE, (0, 2)) # 70%, 0.1\n","  X_E, y_E = get_classifier_data(epochsE)\n","\n","  # score trained model\n","  print(s, ',', pipe_img[s].score(X_E, y_E) * 100)\n","\n","# avg accuracy = 75%"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjt1kLWUy9DH","executionInfo":{"status":"ok","timestamp":1629136795494,"user_tz":-120,"elapsed":380,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"e7981322-b34b-4f35-90c5-1bcbb3964bb7"}},{"cell_type":"code","execution_count":null,"source":["print(\"Train on motor execution & validate on motor imagery\")\n","print(\"Subject,TestAccuracy\")\n","for s in range(7):\n","  # get validation data\n","  datE = get_data(subject = s, experiment = 1)\n","  rawE, epochsE = get_mne_structs(datE, (0, 2)) # 70%, 0.1\n","  X_E, y_E = get_classifier_data(epochsE)\n","\n","  # score trained model\n","  print(s, ',', pipe_motor[s].score(X_E, y_E) * 100)\n","\n","# avg accuracy = 54%"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdRDPs184ID4","executionInfo":{"status":"ok","timestamp":1629136807850,"user_tz":-120,"elapsed":12377,"user":{"displayName":"Eduardo de Conto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2-MNRoAdI5mxkCMBTVXyh_4dtpiZtom1iKN3aNw=s64","userId":"13896657755214125347"}},"outputId":"d218baae-af7f-4619-c7fe-918ac0fa057f"}}]}